
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c5fcc4",
   "metadata": {},
   "source": [
    "## Iteractively refine dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37d4994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T12:38:22.828679Z",
     "start_time": "2023-03-05T12:38:20.588691Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd336e8033b54ab894b7fcef88360858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Instances until next dump:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import typing as t\n",
    "import random\n",
    "import requests\n",
    "import json as json_package\n",
    "import collections\n",
    "import glob\n",
    "\n",
    "import IPython.display\n",
    "import ipywidgets\n",
    "import datasets\n",
    "import tokenizers\n",
    "import ipywidgets\n",
    "import colorama\n",
    "import tqdm\n",
    "\n",
    "import segmentador\n",
    "from config import *\n",
    "import interactive_labeling\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "CN = colorama.Fore.RED\n",
    "CT = colorama.Fore.YELLOW\n",
    "CLN = colorama.Fore.CYAN\n",
    "CSR = colorama.Style.RESET_ALL\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 6000\n",
    "AUTOSAVE_IN_N_INSTANCES = 20\n",
    "\n",
    "\n",
    "DATASET_SPLIT = \"test\"\n",
    "CACHED_INDEX_FILENAME = f\"emendas_variadas_{DATASET_SPLIT}_refined_indices.csv\"\n",
    "SKIPPED_INDEX_FILENAME = f\"emendas_variadas_{DATASET_SPLIT}_skipped_indices.csv\"\n",
    "BRUTE_DATASET_DIR = \"../data\"\n",
    "\n",
    "# TARGET_DATASET_NAME = f\"df_tokenized_split_0_120000_{VOCAB_SIZE}_resplit\"\n",
    "# TARGET_DATASET_NAME = \"ccjs_segmentados_train_test_splits\"\n",
    "TARGET_DATASET_NAME = \"emendas_variadas_segmentadas_train_test_splits\"\n",
    "\n",
    "REFINED_DATASET_DIR = os.path.join(BRUTE_DATASET_DIR, \"refined_datasets\", TARGET_DATASET_NAME)\n",
    "\n",
    "BRUTE_DATASET_URI = os.path.join(BRUTE_DATASET_DIR, TARGET_DATASET_NAME)\n",
    "CACHED_INDEX_URI = os.path.join(REFINED_DATASET_DIR, CACHED_INDEX_FILENAME)\n",
    "SKIPPED_INDEX_URI = os.path.join(REFINED_DATASET_DIR, SKIPPED_INDEX_FILENAME)\n",
    "\n",
    "\n",
    "assert BRUTE_DATASET_URI != REFINED_DATASET_DIR\n",
    "\n",
    "\n",
    "logit_model = segmentador.BERTSegmenter(\n",
    "    uri_model=f\"../segmenter_checkpoint_v2/4_{VOCAB_SIZE}_layer_model_finetuned_emendas\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "pbar_dump = tqdm.auto.tqdm(desc=\"Instances until next dump\", total=AUTOSAVE_IN_N_INSTANCES)\n",
    "it_counter = 0\n",
    "\n",
    "lock_save = False\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce45e6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T12:38:24.196279Z",
     "start_time": "2023-03-05T12:38:24.140301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenizers.Tokenizer.from_file(f\"../tokenizers/{VOCAB_SIZE}_subwords/tokenizer.json\")\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6686a2d",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd40bb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T12:38:26.555033Z",
     "start_time": "2023-03-05T12:38:26.028697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 indices from disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11871\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    with open(CACHED_INDEX_URI, \"r\") as f_index:\n",
    "        cached_indices = set(map(int, f_index.read().split(\",\")))\n",
    "\n",
    "    print(f\"Loaded {len(cached_indices)} indices from disk.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    cached_indices = set()\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(SKIPPED_INDEX_URI, \"r\") as f_index:\n",
    "        skipped_indices = set(map(int, f_index.read().split(\",\")))\n",
    "\n",
    "    print(f\"Loaded {len(skipped_indices)} skipped indices from disk.\")\n",
    "\n",
    "except (FileNotFoundError, ValueError):\n",
    "    skipped_indices = set()\n",
    "\n",
    "\n",
    "new_refined_instances = collections.defaultdict(list)\n",
    "df_brute = datasets.load_from_disk(BRUTE_DATASET_URI)\n",
    "df_brute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82efc8c",
   "metadata": {},
   "source": [
    "## Iteractive refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7012a8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T10:25:34.753394Z",
     "start_time": "2023-02-28T10:25:34.523386Z"
    }
   },
   "outputs": [],
   "source": [
    "df_split = df_brute[DATASET_SPLIT]\n",
    "\n",
    "fn_rand = lambda: random.randint(0, df_split.num_rows)\n",
    "\n",
    "df_view_labels = df_split[\"labels\"]\n",
    "df_view_input_ids = df_split[\"input_ids\"]\n",
    "\n",
    "cls2id = {\n",
    "    \"no-op\": 0,\n",
    "    \"seg\": 1,\n",
    "    \"n-start\": 2,\n",
    "    \"n-end\": 3,\n",
    "}\n",
    "\n",
    "\n",
    "def print_labels(input_ids, labels, input_is_tokens: bool = False):\n",
    "    seg_counter = 1\n",
    "\n",
    "    print(end=CSR)\n",
    "    print(end=f\"{CLN}{seg_counter}.{CSR} \")\n",
    "\n",
    "    if not input_is_tokens:\n",
    "        tokens = list(map(tokenizer.id_to_token, input_ids))\n",
    "\n",
    "    else:\n",
    "        tokens = input_ids\n",
    "\n",
    "    for i, (tok, lab) in enumerate(zip(tokens, labels)):\n",
    "        if lab == cls2id[\"seg\"]:\n",
    "            seg_counter += 1\n",
    "            print(\"\\n\\n\", end=f\"{CLN}{seg_counter}.{CSR} \")\n",
    "\n",
    "        if lab == cls2id[\"n-start\"]:\n",
    "            print(end=CN)\n",
    "\n",
    "        if lab == cls2id[\"n-end\"]:\n",
    "            print(end=CSR)\n",
    "\n",
    "        print(tok, end=\" \")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def dump_refined_dataset():\n",
    "    new_subset = datasets.Dataset.from_dict(new_refined_instances, split=DATASET_SPLIT)\n",
    "    shard_id = len(glob.glob(os.path.join(REFINED_DATASET_DIR, f\"{DATASET_SPLIT}_*\")))\n",
    "    REFINED_DATASET_SHARD_URI = os.path.join(REFINED_DATASET_DIR, f\"{DATASET_SPLIT}_{shard_id}\")\n",
    "    new_subset.save_to_disk(REFINED_DATASET_SHARD_URI)\n",
    "    new_refined_instances.clear()\n",
    "\n",
    "    with open(CACHED_INDEX_URI, \"w\") as f_index:\n",
    "        f_index.write(\",\".join(map(str, sorted(cached_indices))))\n",
    "\n",
    "    with open(SKIPPED_INDEX_URI, \"w\") as f_index:\n",
    "        f_index.write(\",\".join(map(str, sorted(skipped_indices))))\n",
    "\n",
    "    it_counter = 0\n",
    "\n",
    "    print(f\"Saved progress in '{REFINED_DATASET_SHARD_URI}'.\")\n",
    "\n",
    "\n",
    "def fn_run_cell(index_shift: int):\n",
    "    js = IPython.display.Javascript(\n",
    "        f\"Jupyter.notebook.execute_cells([IPython.notebook.get_selected_index()+{index_shift}])\"\n",
    "    )\n",
    "    IPython.display.display(js)\n",
    "\n",
    "\n",
    "def fn_run_last_cell(_):\n",
    "    js = IPython.display.Javascript(\n",
    "        \"Jupyter.notebook.execute_cells([IPython.notebook.ncells() - 16])\"\n",
    "    )\n",
    "    IPython.display.display(js)\n",
    "\n",
    "\n",
    "def fn_skip_instance(_):\n",
    "    if id_ not in cached_indices:\n",
    "        skipped_indices.add(id_)\n",
    "        fn_run_cell(1)\n",
    "\n",
    "\n",
    "button_run_cell = ipywidgets.Button(\n",
    "    description=\"Fetch new instance\",\n",
    "    tooltip=\"Run cell below to fetch a random instance\",\n",
    "    layout=ipywidgets.Layout(width=\"20%\", height=\"48px\", margin=\"0 0 0 5%\"),\n",
    ")\n",
    "button_run_cell.on_click(lambda _: fn_run_cell(1))\n",
    "\n",
    "button_run_cell_b = ipywidgets.Button(\n",
    "    description=\"Fetch refined instance\",\n",
    "    tooltip=\"Fetch refined instance from front-end (run cell below)\",\n",
    "    style=dict(button_color=\"lightgreen\"),\n",
    ")\n",
    "button_run_cell_b.layout = button_run_cell.layout\n",
    "button_run_cell_b.on_click(lambda _: fn_run_cell(1))\n",
    "\n",
    "button_edit_instance = ipywidgets.Button(\n",
    "    description=\"Edit instance\",\n",
    "    tooltip=\"Send instance to interactive front-end for refinement\",\n",
    "    style=dict(button_color=\"lightblue\"),\n",
    ")\n",
    "button_edit_instance.layout = button_run_cell.layout\n",
    "button_edit_instance.on_click(lambda _: fn_run_cell(2))\n",
    "\n",
    "button_save = ipywidgets.Button(\n",
    "    description=\"Save test instance\",\n",
    "    style=dict(button_color=\"salmon\"),\n",
    "    tooltip=\"Run notebook last cell (triggers code to save instance in curated dataset)\",\n",
    ")\n",
    "button_save.layout = button_run_cell.layout\n",
    "button_save.on_click(fn_run_last_cell)\n",
    "\n",
    "button_skip = ipywidgets.Button(\n",
    "    description=\"Skip instance\",\n",
    "    style=dict(button_color=\"black\"),\n",
    "    tooltip=\"Skip instance\",\n",
    ")\n",
    "button_skip.layout = button_run_cell.layout\n",
    "button_skip.on_click(fn_skip_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023d2a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T10:25:39.259268Z",
     "start_time": "2023-02-28T10:25:39.086768Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "\n",
    "m = 140000 if DATASET_SPLIT == \"train\" else 2100\n",
    "k = 0\n",
    "cached_margins_filename = f\"cached_margins_emendas_variadas_{DATASET_SPLIT}_{m}_{k}.txt\"\n",
    "APPLY_ACTIVE_LEARNING = False\n",
    "\n",
    "\n",
    "if APPLY_ACTIVE_LEARNING and DATASET_SPLIT != \"test\":\n",
    "    logit_model._model = torch.load(\"bert_v2_4_layers_logit_model_finetuned_emendas7.pt\")\n",
    "    logit_model.model.to(\"cuda\")\n",
    "    logit_model.device = \"cuda\"\n",
    "    \n",
    "    if os.path.exists(cached_margins_filename):\n",
    "        with open(cached_margins_filename, \"r\") as f_in:\n",
    "            margins = np.asfarray(f_in.readlines())\n",
    "\n",
    "    else:\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        margins = np.full(len(df_view_input_ids), fill_value=np.inf)\n",
    "\n",
    "        compute_diff_tokens = False\n",
    "        diff_80 = 0\n",
    "        total_tokens_80 = 1e-8\n",
    "\n",
    "        pbar = tqdm.auto.tqdm(\n",
    "            enumerate(df_view_input_ids[-1 - k : -m - 1 - k : -1], 1 + k),\n",
    "            total=min(m, len(df_view_input_ids) - k),\n",
    "        )\n",
    "\n",
    "        for i, text in pbar:\n",
    "            logits = logit_model(\n",
    "                tokenizer.decode(text),\n",
    "                batch_size=128,\n",
    "                return_logits=True,\n",
    "                regex_justificativa=\"@@@@@@\",\n",
    "            ).logits\n",
    "            \n",
    "            probs = scipy.special.softmax(logits, axis=-1)\n",
    "            margin = np.diff(np.sort(probs, axis=-1)[:, [-2, -1]]).ravel()\n",
    "\n",
    "            try:\n",
    "                true_labels = np.asarray(df_view_labels[-i], dtype=int)\n",
    "                not_middle_word = true_labels != -100\n",
    "\n",
    "                if compute_diff_tokens:\n",
    "                    try:\n",
    "                        margin_80_inds = np.flatnonzero(\n",
    "                            np.logical_and(not_middle_word, margin >= 0.90)\n",
    "                        )\n",
    "                        high_conf_preds = np.argmax(probs[margin_80_inds], axis=-1)\n",
    "                        diff_inds = np.flatnonzero(true_labels[margin_80_inds] != high_conf_preds)\n",
    "                        diff_80 += diff_inds.size\n",
    "                        total_tokens_80 += margin_80_inds.size\n",
    "\n",
    "                        if diff_inds.size:\n",
    "                            new_labels.append(\n",
    "                                (\n",
    "                                    i,\n",
    "                                    list(\n",
    "                                        zip(margin_80_inds[diff_inds], high_conf_preds[diff_inds])\n",
    "                                    ),\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                margin = margin[not_middle_word]\n",
    "\n",
    "            except IndexError as err:\n",
    "                margin = [np.inf, np.inf]\n",
    "\n",
    "            try:\n",
    "                margins[-i] = float(np.quantile(margin, 0.01))\n",
    "            except IndexError:\n",
    "                margins[-i] = 0.0\n",
    "\n",
    "            if compute_diff_tokens:\n",
    "                pbar.set_description(\n",
    "                    f\"50: {100. * diff_80 / total_tokens_80:.2f}% ({diff_80} of {int(total_tokens_80)})\"\n",
    "                )\n",
    "\n",
    "    ids_to_fetch = np.argsort(margins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7944f79e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T15:51:34.712299Z",
     "start_time": "2023-02-27T15:51:34.687870Z"
    }
   },
   "outputs": [],
   "source": [
    "if APPLY_ACTIVE_LEARNING and not os.path.exists(cached_margins_filename):\n",
    "    with open(cached_margins_filename, \"w\") as f_out:\n",
    "        f_out.write(\"\\n\".join(map(lambda x: f\"{x:.6f}\", margins)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cce84d",
   "metadata": {},
   "source": [
    "## Interative refinery with low margin (active learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,