{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4890b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T13:57:23.914010Z",
     "start_time": "2023-03-03T13:57:21.621211Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as tF\n",
    "import pytorch_lightning as pl\n",
    "import tokenizers\n",
    "import datasets\n",
    "\n",
    "\n",
    "DEBUG_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e7dfc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T13:57:23.918031Z",
     "start_time": "2023-03-03T13:57:23.915293Z"
    }
   },
   "outputs": [],
   "source": [
    "class HFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hfdf):\n",
    "        self.hfdf = hfdf\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.hfdf[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cae9a71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T15:56:43.660235Z",
     "start_time": "2023-03-03T15:56:43.639417Z"
    }
   },
   "outputs": [],
   "source": [
    "class LitSegmenterBaseline(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        tokenizer_uri: str,\n",
    "        dataset_uri: str,\n",
    "        batch_size: int,\n",
    "        num_layers: int = 1,\n",
    "        bidirectional: bool = True,\n",
    "        num_classes: int = 4,\n",
    "        pad_token: str = \"[PAD]\",\n",
    "    ):\n",
    "        super(LitSegmenterBaseline, self).__init__()\n",
    "\n",
    "        self.tokenizer = tokenizers.Tokenizer.from_file(tokenizer_uri)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_id = self.tokenizer.get_vocab().get(pad_token, 0)\n",
    "\n",
    "        def fn_pad_sequences(batch):\n",
    "            X = [torch.tensor(x_i[\"input_ids\"], dtype=torch.int) for x_i in batch]\n",
    "            y = [torch.tensor(y_i[\"labels\"]) for y_i in batch]\n",
    "\n",
    "            X = nn.utils.rnn.pad_sequence(X, padding_value=self.pad_id, batch_first=True)\n",
    "            y = nn.utils.rnn.pad_sequence(y, padding_value=-100, batch_first=True)\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        self.fn_pad_sequences = fn_pad_sequences\n",
    "\n",
    "        if isinstance(dataset_uri, str):\n",
    "            self.hfdf = datasets.load_from_disk(dataset_uri)\n",
    "            \n",
    "        else:\n",
    "            dfs = []\n",
    "            for uri in dataset_uri:\n",
    "                dfs.append(datasets.load_from_disk(uri))\n",
    "            \n",
    "            hfdf = {}\n",
    "            for key in dfs[0].keys():\n",
    "                hfdf[key] = datasets.concatenate_datasets([df[key] for df in dfs])\n",
    "            \n",
    "            self.hfdf = datasets.DatasetDict(hfdf)\n",
    "\n",
    "        print(self.hfdf)\n",
    "            \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=self.tokenizer.get_vocab_size(),\n",
    "            embedding_dim=768,\n",
    "            padding_idx=self.pad_id,\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0 if num_layers == 1 else 0.1,\n",
    "            bidirectional=bidirectional,\n",
    "            proj_size=0,\n",
    "        )\n",
    "\n",
    "        self.lin_out = nn.Linear(\n",
    "            (1 + int(bidirectional)) * hidden_size,\n",
    "            num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "\n",
    "        if isinstance(out, str):\n",
    "            out = self.tokenizer(out, return_tensors=\"pt\")\n",
    "            out = out[\"input_ids\"]\n",
    "\n",
    "        out = self.embeddings(out)\n",
    "        out, *_ = self.lstm(out)\n",
    "        out = self.lin_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_pred_metrics(y_preds, y, phase: str) -> dict[str, float]:\n",
    "        y_preds = y_preds.view(-1, y_preds.shape[-1])\n",
    "        y = y.view(-1).squeeze()\n",
    "\n",
    "        loss = F.cross_entropy(input=y_preds, target=y, ignore_index=-100)\n",
    "\n",
    "        non_pad_inds = [i for i, cls_i in enumerate(y) if cls_i != -100]\n",
    "\n",
    "        per_cls_recall = tF.recall(\n",
    "            preds=y_preds[non_pad_inds, ...],\n",
    "            target=y[non_pad_inds],\n",
    "            num_classes=4,\n",
    "            average=None,\n",
    "        )\n",
    "\n",
    "        per_cls_precision = tF.precision(\n",
    "            preds=y_preds[non_pad_inds, ...],\n",
    "            target=y[non_pad_inds],\n",
    "            num_classes=4,\n",
    "            average=None,\n",
    "        )\n",
    "\n",
    "        macro_precision = float(per_cls_precision.mean().item())\n",
    "        macro_recall = float(per_cls_recall.mean().item())\n",
    "        macro_f1_score = (\n",
    "            2.0 * macro_precision * macro_recall / (1e-8 + macro_precision + macro_recall)\n",
    "        )\n",
    "\n",
    "        out = {\n",
    "            f\"{(phase + '_') if phase != 'train' else ''}loss\": loss,\n",
    "            **{f\"{phase}_cls_{i}_precision\": float(val) for i, val in enumerate(per_cls_precision)},\n",
    "            **{f\"{phase}_cls_{i}_recall\": float(val) for i, val in enumerate(per_cls_recall)},\n",
    "            f\"{phase}_macro_precision\": macro_precision,\n",
    "            f\"{phase}_macro_recall\": macro_recall,\n",
    "            f\"{phase}_macro_f1_score\": macro_f1_score,\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _agg_stats(step_outputs):\n",
    "        out = {}\n",
    "        agg_items = collections.defaultdict(list)\n",
    "\n",
    "        for items in step_outputs:\n",
    "            for key, val in items.items():\n",
    "                if not isinstance(val, torch.Tensor):\n",
    "                    val = torch.tensor(val)\n",
    "\n",
    "                agg_items[key].append(val)\n",
    "\n",
    "        for key, vals in agg_items.items():\n",
    "            avg_vals = float(torch.stack(vals).mean().item())\n",
    "            out[f\"avg_{key}\"] = avg_vals\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        X, y = batch\n",
    "        y_preds = self.forward(X)\n",
    "\n",
    "        out = self._compute_pred_metrics(y_preds, y, phase=\"train\")\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        out = self._agg_stats(training_step_outputs)\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx: int):\n",
    "#         X, y = batch\n",
    "#         y_preds = self.forward(X)\n",
    "\n",
    "#         out = self._compute_pred_metrics(y_preds, y, phase=\"val\")\n",
    "\n",
    "#         self.log_dict(\n",
    "#             out,\n",
    "#             on_step=False,\n",
    "#             on_epoch=True,\n",
    "#             logger=True,\n",
    "#         )\n",
    "\n",
    "#         return out\n",
    "\n",
    "#     def validation_epoch_end(self, validation_step_outputs):\n",
    "#         out = self._agg_stats(validation_step_outputs)\n",
    "\n",
    "#         self.log_dict(\n",
    "#             out,\n",
    "#             on_step=False,\n",
    "#             on_epoch=True,\n",
    "#             logger=True,\n",
    "#         )\n",
    "\n",
    "    def test_step(self, batch, batch_idx: int):\n",
    "        X, y = batch\n",
    "        y_preds = self.forward(X)\n",
    "\n",
    "        out = self._compute_pred_metrics(y_preds, y, phase=\"test\")\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        out = self._agg_stats(test_step_outputs)\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.75)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        df_train = HFDataset(self.hfdf[\"train\"])\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset=df_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            collate_fn=self.fn_pad_sequences,\n",
    "        )\n",
    "\n",
    "        return train_dataloader\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         df_eval = HFDataset(self.hfdf[\"eval\"])\n",
    "\n",
    "#         eval_dataloader = torch.utils.data.DataLoader(\n",
    "#             dataset=df_eval,\n",
    "#             batch_size=self.batch_size,\n",
    "#             shuffle=False,\n",
    "#             num_workers=8,\n",
    "#             collate_fn=self.fn_pad_sequences,\n",
    "#         )\n",
    "\n",
    "#         return eval_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        df_test = HFDataset(self.hfdf[\"test\"])\n",
    "\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset=df_test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            collate_fn=self.fn_pad_sequences,\n",
    "        )\n",
    "\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de7693b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T08:08:18.835017Z",
     "start_time": "2023-03-03T20:31:23.367990Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | embeddings | Embedding | 4.6 M \n",
      "1 | lstm       | LSTM      | 2.1 M \n",
      "2 | lin_out    | Linear    | 2.1 K \n",
      "-----------------------------------------\n",
      "6.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 M     Total params\n",
      "26.845    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 159808\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2602\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b16579df682440f9468681f47016f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nvme/segmentador/venvs/env3.9.10/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:207: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "/media/nvme/segmentador/venvs/env3.9.10/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1398: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at /media/nvme/segmentador/notebooks/lightning_logs/version_15/checkpoints/epoch=9-step=12489.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /media/nvme/segmentador/notebooks/lightning_logs/version_15/checkpoints/epoch=9-step=12489.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1b3b1f8aaf4c798d4d5b0c396885e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_cls_0_precision': 0.998159646987915,\n",
      " 'avg_test_cls_0_recall': 0.999201774597168,\n",
      " 'avg_test_cls_1_precision': 0.9796888828277588,\n",
      " 'avg_test_cls_1_recall': 0.9497243165969849,\n",
      " 'avg_test_cls_2_precision': 0.9082029461860657,\n",
      " 'avg_test_cls_2_recall': 0.8206210136413574,\n",
      " 'avg_test_cls_3_precision': 0.6968904733657837,\n",
      " 'avg_test_cls_3_recall': 0.5787816643714905,\n",
      " 'avg_test_loss': 0.01663857512176037,\n",
      " 'avg_test_macro_f1_score': 0.8640515208244324,\n",
      " 'avg_test_macro_precision': 0.8957353830337524,\n",
      " 'avg_test_macro_recall': 0.8370822072029114,\n",
      " 'test_cls_0_precision': 0.998175323009491,\n",
      " 'test_cls_0_recall': 0.9992001056671143,\n",
      " 'test_cls_1_precision': 0.9800203442573547,\n",
      " 'test_cls_1_recall': 0.95093834400177,\n",
      " 'test_cls_2_precision': 0.9074270725250244,\n",
      " 'test_cls_2_recall': 0.8203122019767761,\n",
      " 'test_cls_3_precision': 0.6964413523674011,\n",
      " 'test_cls_3_recall': 0.580051839351654,\n",
      " 'test_loss': 0.01651456579566002,\n",
      " 'test_macro_f1_score': 0.8642488718032837,\n",
      " 'test_macro_precision': 0.8955160975456238,\n",
      " 'test_macro_recall': 0.8376255631446838}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    configs = [\n",
    "#         (512, 32),\n",
    "        (256, 32),\n",
    "#         (128, 64),\n",
    "#         (64, 64),\n",
    "#         (32, 64),\n",
    "    ]\n",
    "\n",
    "    for hidden_size, batch_size in configs:\n",
    "        accumulate_grad_batches = 128 // batch_size\n",
    "\n",
    "        model = LitSegmenterBaseline(\n",
    "            hidden_size=hidden_size,\n",
    "            batch_size=batch_size,\n",
    "            tokenizer_uri=\"../tokenizers/6000_subwords/tokenizer.json\",\n",
    "            dataset_uri=[\n",
    "                \"./final_curated_dataset_for_training\",\n",
    "                \"../data/refined_datasets/ccjs_segmentados_train_test_splits/ccjs_segmentados_train_test_splits_curados\",\n",
    "                \"../data/refined_datasets/emendas_variadas_segmentadas_train_test_splits/emendas_variadas_train_test_splits\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer.from_argparse_args(\n",
    "            args,\n",
    "            overfit_batches=0.001 if DEBUG_RUN else 0.0,\n",
    "            accumulate_grad_batches=accumulate_grad_batches,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model)\n",
    "\n",
    "        if not DEBUG_RUN:\n",
    "            trainer.test()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "\n",
    "    args = parser.parse_args(\n",
    "        \"\"\"\n",
    "        --gpu 1\n",
    "        --max_epochs 10\n",
    "        --log_every_n_steps 1000\n",
    "        --precision 32\n",
    "    \"\"\".split()\n",
    "    )\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4bfd2",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## 512 hidden units (lr_scheduler etc)\n",
    "{'avg_test_cls_0_precision': 0.9982964396476746,\n",
    " 'avg_test_cls_0_recall': 0.9993011951446533,\n",
    " 'avg_test_cls_1_precision': 0.9845898747444153,