{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed1bdd7",
   "metadata": {},
   "source": [
    "# Notebook to train segmenter model\n",
    "\n",
    "This notebook uses the previously created tokenizer and labeled data to effectively train Transformer Encoder models to segment Brazilian legal text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458ccb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:16:51.388980Z",
     "start_time": "2023-01-24T19:16:51.370039Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn\n",
    "import datasets\n",
    "\n",
    "try:\n",
    "    from segmentador import *\n",
    "\n",
    "except ImportError:\n",
    "    from src import *\n",
    "\n",
    "\n",
    "import eval_model\n",
    "\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "PREDICT_TEST_SET_AT_END = True\n",
    "DEBUG_RUN = False\n",
    "RESOURCE_DIR = \"..\"\n",
    "\n",
    "\n",
    "USE_FP16 = False\n",
    "DEVICE = \"cuda\"\n",
    "LOCAL_FILES_ONLY = True\n",
    "LOAD_PRETRAINED_WEIGHTS = True\n",
  